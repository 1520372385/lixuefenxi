"""
ç›¸ä¼¼åº¦åˆ†ææ¨¡å— - åŸºäºyizhixing.py
å®ç°Esimç›¸ä¼¼åº¦åˆ†æï¼Œä½¿ç”¨å‚è€ƒæ•°æ®é›†å¹³å‡å€¼ä½œä¸ºåŸºå‡†å‘é‡
åŒ…å«å®Œæ•´çš„æµ‹ç‚¹åˆ‡æ¢å’Œé¢œè‰²æ ‡è®°åŠŸèƒ½
"""

import streamlit as st
import pandas as pd
import numpy as np
import plotly.graph_objects as go
from scipy import interpolate
import io
import re
from PIL import Image

# é¡µé¢é…ç½®
st.set_page_config(
    page_title="ç›¸ä¼¼åº¦åˆ†æ - åŠ›å­¦æŒ¯åŠ¨æ•°æ®ä¸€è‡´æ€§åˆ†æç³»ç»Ÿ",
    page_icon="ğŸ“Š",
    layout="wide",
    initial_sidebar_state="expanded"
)

# åº”ç”¨æ ‡é¢˜
st.title("ğŸ“Š ç›¸ä¼¼åº¦åˆ†ææ¨¡å—")
st.markdown("Esimç›¸ä¼¼åº¦åˆ†æ - æ–°æ•°æ® vs å‚è€ƒæ•°æ®é›†å¹³å‡å€¼")
st.markdown("---")

# è½¬æ¢å‡½æ•°ï¼šä¸ä¸€è‡´æ€§åˆ†ææ¨¡å—ç›¸åŒçš„é€»è¾‘
@st.cache_data
def convert_lms_excel(file):
    """
    - è¯»å–ç¬¬ä¸€ä¸ªå·¥ä½œè¡¨ï¼ˆä¸è®¾headerï¼‰
    - ç¬¬12è¡Œï¼ˆç´¢å¼•11ï¼‰è·å–æµ‹ç‚¹åç§°ï¼Œå–å¶æ•°åˆ—ï¼ˆç´¢å¼•1,3,5...ï¼‰
    - ç¬¬ä¸€åˆ—ä¸ºé¢‘ç‡ï¼ˆç´¢å¼•0ï¼‰ï¼Œæ•°æ®ä»ç¬¬13è¡Œï¼ˆç´¢å¼•12ï¼‰å¼€å§‹
    - æµ‹ç‚¹åä¿ç•™ XM/YM/ZM + æ•°å­—ï¼ˆå¦‚ XM1ã€YM2ã€ZM3ï¼‰
    - è¾“å‡º DataFrame: ç¬¬ä¸€åˆ— 'HZ' + æµ‹ç‚¹åˆ—
    """
    try:
        excel_file = pd.ExcelFile(file)
        sheet_names = excel_file.sheet_names
        df = pd.read_excel(file, sheet_name=sheet_names[0], header=None)

        row12 = df.iloc[11]
        measurement_indices = [i for i in range(1, len(row12), 2) if pd.notna(row12[i])]
        measurement_names = [str(row12[i]).strip() for i in measurement_indices]

        frequency_col = 0
        data_start_row = 12
        df_data = df.iloc[data_start_row:]

        processed_df = pd.DataFrame()
        processed_df['HZ'] = df_data[frequency_col].reset_index(drop=True)

        for idx, name in zip(measurement_indices, measurement_names):
            match = re.search(r'(XM\d+|YM\d+|ZM\d+)', name)
            point_name = match.group(1) if match else name
            processed_df[point_name] = df_data[idx].reset_index(drop=True)

        processed_df = processed_df.dropna(how='all')
        processed_df['HZ'] = pd.to_numeric(processed_df['HZ'], errors='coerce')
        for col in processed_df.columns[1:]:
            processed_df[col] = pd.to_numeric(processed_df[col], errors='coerce')
        processed_df = processed_df.dropna()
        return processed_df
    except Exception:
        return None

@st.cache_data
def read_processed_excel(file):
    """
    è¯»å–å·²å¤„ç†å‚è€ƒæ•°æ®ï¼ˆç¬¬ä¸€åˆ—ä¸ºé¢‘ç‡ï¼Œå…¶ä»–åˆ—ä¸ºæµ‹ç‚¹ï¼‰ã€‚
    """
    try:
        excel_file = pd.ExcelFile(file)
        sheet_name = excel_file.sheet_names[0]
        df = pd.read_excel(file, sheet_name=sheet_name)
        df.iloc[:, 0] = pd.to_numeric(df.iloc[:, 0], errors='coerce')
        for c in df.columns[1:]:
            df[c] = pd.to_numeric(df[c], errors='coerce')
        df = df.dropna()
        return df
    except Exception:
        return None

@st.cache_data
def load_location_data(_location_file):
    """åŠ è½½æµ‹ç‚¹ä½ç½®è¡¨æ•°æ®"""
    try:
        # è¯»å–ä½ç½®è¡¨
        location_df = pd.read_excel(_location_file)
        
        # æ£€æŸ¥å¿…è¦çš„åˆ—æ˜¯å¦å­˜åœ¨
        required_columns = ['æµ‹ç‚¹åç§°', 'èˆ±æ¿', 'å•æœºåŒºåŸŸ']
        missing_columns = [col for col in required_columns if col not in location_df.columns]
        
        if missing_columns:
            st.error(f"æµ‹ç‚¹ä½ç½®è¡¨ç¼ºå°‘å¿…è¦çš„åˆ—: {missing_columns}")
            return {}
        
        # åˆ›å»ºæµ‹ç‚¹ä½ç½®æ˜ å°„å­—å…¸
        location_data = {}
        for _, row in location_df.iterrows():
            point_name = str(row['æµ‹ç‚¹åç§°']).strip()
            cabin = str(row['èˆ±æ¿']).strip()
            area = str(row['å•æœºåŒºåŸŸ']).strip()
            location_data[point_name] = f"{cabin}-{area}"
        
        return location_data
        
    except Exception as e:
        st.error(f"æµ‹ç‚¹ä½ç½®è¡¨è¯»å–é”™è¯¯: {str(e)}")
        return {}

def get_point_location(point_name):
    """è·å–æµ‹ç‚¹çš„ä½ç½®ä¿¡æ¯"""
    # ä»æµ‹ç‚¹åç§°ä¸­æå–æ•°å­—éƒ¨åˆ†ï¼ˆä¾‹å¦‚ï¼šä»"XM1"ä¸­æå–"1"ï¼‰
    match = re.search(r'\d+', point_name)
    if match:
        point_number = match.group()  # æå–åˆ°çš„æ•°å­—ï¼ˆå­—ç¬¦ä¸²æ ¼å¼ï¼‰
        
        # ç”¨æ•°å­—å»ä½ç½®è¡¨ä¸­æŸ¥æ‰¾
        if point_number in st.session_state.location_data:
            return st.session_state.location_data[point_number]
    
    return "æœªçŸ¥ä½ç½®"

def format_point_with_location(point_name):
    """æ ¼å¼åŒ–æµ‹ç‚¹ä¿¡æ¯ï¼Œæ·»åŠ ä½ç½®ä¿¡æ¯"""
    location = get_point_location(point_name)
    return f"{point_name} ({location})"

def format_point_with_location_and_similarity(point_info):
    """æ ¼å¼åŒ–æµ‹ç‚¹ä¿¡æ¯ï¼Œæ·»åŠ ä½ç½®ä¿¡æ¯å’Œç›¸ä¼¼åº¦å€¼"""
    # æå–æµ‹ç‚¹åç§°ï¼ˆå»æ‰ç›¸ä¼¼åº¦å€¼ï¼‰
    if '(' in point_info and ')' in point_info:
        # æ ¼å¼å¦‚: "XM1(0.852)"
        point_name = point_info.split('(')[0]
        similarity_part = point_info[len(point_name):]
    else:
        point_name = point_info
        similarity_part = ""
    
    # è·å–ä½ç½®ä¿¡æ¯
    location = get_point_location(point_name)
    
    # æ ¼å¼åŒ–è¾“å‡º
    return f"{point_name}({location}){similarity_part}"

def linear_interpolation(data_dict, target_frequencies):
    """
    çº¿æ€§æ’å€¼å‡½æ•°ï¼Œå°†æ‰€æœ‰æ•°æ®æ’å€¼åˆ°ç›¸åŒçš„é¢‘ç‡ç‚¹ä¸Š
    åªè¿›è¡Œå†…æ’ï¼Œä¸å¤–æ¨ï¼ŒèŒƒå›´å¤–å¡«å……NaN
    """
    interpolated_data = {}
    
    # éªŒè¯ç›®æ ‡é¢‘ç‡æ•°ç»„
    if target_frequencies is None or len(target_frequencies) == 0:
        st.error("ç›®æ ‡é¢‘ç‡æ•°ç»„ä¸ºç©ºæˆ–æ— æ•ˆ")
        return interpolated_data
    
    for label, df in data_dict.items():
        # è·å–åŸå§‹é¢‘ç‡å’Œæ•°æ®
        original_freq = df.iloc[:, 0].values
        
        # éªŒè¯åŸå§‹æ•°æ®
        if len(original_freq) == 0:
            st.warning(f"æ•°æ® '{label}' çš„é¢‘ç‡æ•°æ®ä¸ºç©ºï¼Œè·³è¿‡è¯¥æ•°æ®")
            continue
            
        interpolated_df = pd.DataFrame()
        interpolated_df['HZ'] = target_frequencies
        
        # å¯¹æ¯ä¸ªæµ‹ç‚¹è¿›è¡Œæ’å€¼
        for col in df.columns[1:]:
            original_data = df[col].values
            
            # éªŒè¯æµ‹ç‚¹æ•°æ®
            if len(original_data) == 0:
                # å¦‚æœæµ‹ç‚¹æ•°æ®ä¸ºç©ºï¼Œå¡«å……NaN
                interpolated_df[col] = np.full(len(target_frequencies), np.nan)
                continue
            
            try:
                # åˆ›å»ºæ’å€¼å‡½æ•°ï¼ˆåªå†…æ’ï¼Œä¸å¤–æ¨ï¼‰
                f = interpolate.interp1d(original_freq, original_data, 
                                       kind='linear', 
                                       bounds_error=False, 
                                       fill_value=np.nan)  # èŒƒå›´å¤–å¡«å……NaN
                
                # æ’å€¼åˆ°ç›®æ ‡é¢‘ç‡ç‚¹
                interpolated_data_col = f(target_frequencies)
                
                # ç¡®ä¿æ’å€¼ç»“æœé•¿åº¦ä¸ç›®æ ‡é¢‘ç‡ä¸€è‡´
                if len(interpolated_data_col) != len(target_frequencies):
                    # å¦‚æœé•¿åº¦ä¸åŒ¹é…ï¼Œåˆ›å»ºæ­£ç¡®é•¿åº¦çš„æ•°ç»„å¹¶å¡«å……NaN
                    interpolated_data_col = np.full(len(target_frequencies), np.nan)
                    # é‡æ–°æ’å€¼ï¼Œç¡®ä¿é•¿åº¦æ­£ç¡®
                    try:
                        interpolated_data_col = f(target_frequencies)
                    except:
                        # å¦‚æœæ’å€¼å¤±è´¥ï¼Œä¿æŒNaNæ•°ç»„
                        pass
                
                interpolated_df[col] = interpolated_data_col
                
            except Exception as e:
                # å¦‚æœæ’å€¼å¤±è´¥ï¼Œå¡«å……NaN
                st.warning(f"æµ‹ç‚¹ '{col}' æ’å€¼å¤±è´¥: {str(e)}")
                interpolated_df[col] = np.full(len(target_frequencies), np.nan)
        
        interpolated_data[label] = interpolated_df
    
    return interpolated_data

def esim_similarity(x, y, weights=None):
    """
    è®¡ç®—Esimç›¸ä¼¼åº¦
    å…¬å¼: s_Esim(X,Y) = 1/n * Î£ Ï‰_i * e^(-|x_i-y_i|/(|x_i-y_i|+|x_i+y_i|/2))
    ä½¿ç”¨ç­‰æƒé‡1
    """
    n = len(x)
    similarity_sum = 0
    
    # å¦‚æœæ²¡æœ‰æä¾›æƒé‡ï¼Œé»˜è®¤ä½¿ç”¨ç­‰æƒé‡
    if weights is None:
        weights = np.ones(n)
    
    for i in range(n):
        diff = abs(x[i] - y[i])
        denominator = diff + abs(x[i] + y[i]) / 2
        if denominator == 0:
            # é¿å…é™¤é›¶é”™è¯¯ï¼Œå½“ä¸¤è€…éƒ½ä¸º0æ—¶è®¤ä¸ºå®Œå…¨ç›¸ä¼¼
            similarity_sum += weights[i]
        else:
            similarity_sum += weights[i] * np.exp(-diff / denominator)
    
    return similarity_sum / n

def calculate_similarity_matrix(new_dict, ref_dict, measurement_points):
    """
    è®¡ç®—ç›¸ä¼¼åº¦çŸ©é˜µ
    XåŸºå‡†å‘é‡ï¼šå‚è€ƒæ•°æ®é›†å¹³å‡å€¼
    Yå‘é‡ï¼šæ–°æ•°æ®æµ‹ç‚¹æ•°æ®
    """
    # æ£€æŸ¥æ˜¯å¦æœ‰å‚è€ƒæ•°æ®
    if not ref_dict:
        st.error("æ²¡æœ‰å‚è€ƒæ•°æ®ï¼Œæ— æ³•è®¡ç®—ç›¸ä¼¼åº¦")
        return None, None
    
    # è‡ªåŠ¨æ‰¾åˆ°æ•°æ®ç‚¹æœ€å¤šçš„å‚è€ƒæ•°æ®ä½œä¸ºåŸºå‡†
    max_points = 0
    target_frequencies = None
    max_ref_label = None
    
    for label, df in ref_dict.items():
        if df is not None:
            num_points = len(df)
            if num_points > max_points:
                max_points = num_points
                target_frequencies = df.iloc[:, 0].values  # é¢‘ç‡åˆ—
                max_ref_label = label
    
    if target_frequencies is None:
        st.error("æ²¡æœ‰æ‰¾åˆ°æœ‰æ•ˆçš„å‚è€ƒæ•°æ®")
        return None, None
    
    st.info(f"ä½¿ç”¨ '{max_ref_label}' å‚è€ƒæ•°æ®çš„ {max_points} ä¸ªé¢‘ç‡ç‚¹ä½œä¸ºæ’å€¼åŸºå‡†")
    
    # åˆå¹¶æ–°æ•°æ®å’Œå‚è€ƒæ•°æ®è¿›è¡Œæ’å€¼
    all_data_dict = {**new_dict, **ref_dict}
    interpolated_data = linear_interpolation(all_data_dict, target_frequencies)
    
    # è®¡ç®—ç›¸ä¼¼åº¦çŸ©é˜µ
    similarity_matrix = pd.DataFrame(index=list(new_dict.keys()), columns=measurement_points)
    
    # ä½¿ç”¨å‚è€ƒæ•°æ®é›†å¹³å‡å€¼ä½œä¸ºåŸºå‡†å‘é‡
    x_reference = {}
    
    # è®¡ç®—å‚è€ƒæ•°æ®é›†çš„å¹³å‡å€¼
    for point in measurement_points:
        # æ”¶é›†æ‰€æœ‰å‚è€ƒæ•°æ®çš„è¯¥æµ‹ç‚¹æ•°æ®
        all_ref_data = []
        
        for label, df in interpolated_data.items():
            if label in ref_dict and point in df:
                valid_data = df[point].dropna().values
                if len(valid_data) > 0:
                    all_ref_data.append(valid_data)
        
        # å¦‚æœæ”¶é›†åˆ°äº†å‚è€ƒæ•°æ®ï¼Œè®¡ç®—å¹³å‡å€¼
        if all_ref_data:
            # æ‰¾åˆ°æœ€çŸ­çš„æ•°æ®é•¿åº¦ï¼ˆç¡®ä¿æ‰€æœ‰å‘é‡é•¿åº¦ä¸€è‡´ï¼‰
            min_length = min(len(data) for data in all_ref_data)
            
            # æˆªå–æ‰€æœ‰æ•°æ®åˆ°ç›¸åŒé•¿åº¦å¹¶è®¡ç®—å¹³å‡å€¼
            trimmed_data = [data[:min_length] for data in all_ref_data]
            average_data = np.mean(trimmed_data, axis=0)
            
            if len(average_data) > 0:
                x_reference[point] = average_data
    
    # ä¿å­˜åŸºå‡†å‘é‡æ•°æ®åˆ°session state
    st.session_state.x_reference = x_reference
    st.session_state.target_frequencies = target_frequencies
    
    # è®¡ç®—æ¯ä¸ªæ–°æ•°æ®æ–‡ä»¶æ¯ä¸ªæµ‹ç‚¹çš„ç›¸ä¼¼åº¦ï¼ˆä¸å‚è€ƒæ•°æ®é›†å¹³å‡å€¼æ¯”è¾ƒï¼‰
    for label in new_dict.keys():
        if label in interpolated_data:
            for point in measurement_points:
                if (point in interpolated_data[label] and point in x_reference):
                    y_data = interpolated_data[label][point].dropna().values
                    x_data = x_reference[point]
                    
                    # ç¡®ä¿å‘é‡é•¿åº¦åŒ¹é…ï¼Œä½¿ç”¨ç›¸åŒé•¿åº¦çš„æ•°æ®
                    min_length = min(len(x_data), len(y_data))
                    if min_length > 0:
                        x_trimmed = x_data[:min_length]
                        y_trimmed = y_data[:min_length]
                        
                        # å†æ¬¡æ£€æŸ¥é•¿åº¦æ˜¯å¦ä¸€è‡´
                        if len(x_trimmed) == len(y_trimmed):
                            # ä½¿ç”¨ç­‰æƒé‡1
                            similarity = esim_similarity(x_trimmed, y_trimmed)
                            similarity_matrix.loc[label, point] = similarity
                        else:
                            # å¦‚æœé•¿åº¦ä»ç„¶ä¸åŒ¹é…ï¼Œè®°å½•NaN
                            similarity_matrix.loc[label, point] = np.nan
                    else:
                        similarity_matrix.loc[label, point] = np.nan
                else:
                    similarity_matrix.loc[label, point] = np.nan
    
    return similarity_matrix, interpolated_data

def plot_similarity_results(similarity_matrix):
    """
    ç»˜åˆ¶ç›¸ä¼¼åº¦ç»“æœå›¾è¡¨ï¼ˆXã€Yã€Zæµ‹ç‚¹åˆ†åˆ«ï¼‰
    """
    # åˆ†ç¦»Xã€Yã€Zæµ‹ç‚¹
    x_points = [p for p in similarity_matrix.columns if p.startswith('XM')]
    y_points = [p for p in similarity_matrix.columns if p.startswith('YM')]
    z_points = [p for p in similarity_matrix.columns if p.startswith('ZM')]
    
    # æ’åºæµ‹ç‚¹
    x_points.sort(key=lambda x: int(x[2:]))
    y_points.sort(key=lambda x: int(x[2:]))
    z_points.sort(key=lambda x: int(x[2:]))
    
    # åˆ›å»ºå›¾è¡¨
    fig = go.Figure()
    
    # Xæµ‹ç‚¹å›¾è¡¨
    for file_name in similarity_matrix.index:
        x_values = [int(p[2:]) for p in x_points]
        y_values = [similarity_matrix.loc[file_name, p] for p in x_points]
        fig.add_trace(go.Scatter(
            x=x_values, y=y_values, mode='lines+markers',
            name=f"{file_name} - Xæµ‹ç‚¹", line=dict(width=2), marker=dict(size=4)
        ))
    
    # Yæµ‹ç‚¹å›¾è¡¨
    for file_name in similarity_matrix.index:
        x_values = [int(p[2:]) for p in y_points]
        y_values = [similarity_matrix.loc[file_name, p] for p in y_points]
        fig.add_trace(go.Scatter(
            x=x_values, y=y_values, mode='lines+markers',
            name=f"{file_name} - Yæµ‹ç‚¹", line=dict(width=2), marker=dict(size=4)
        ))
    
    # Zæµ‹ç‚¹å›¾è¡¨
    for file_name in similarity_matrix.index:
        x_values = [int(p[2:]) for p in z_points]
        y_values = [similarity_matrix.loc[file_name, p] for p in z_points]
        fig.add_trace(go.Scatter(
            x=x_values, y=y_values, mode='lines+markers',
            name=f"{file_name} - Zæµ‹ç‚¹", line=dict(width=2), marker=dict(size=4)
        ))
    
    fig.update_layout(
        title="Esimç›¸ä¼¼åº¦åˆ†æç»“æœ",
        xaxis_title="æµ‹ç‚¹ç¼–å·",
        yaxis_title="Esimç›¸ä¼¼åº¦",
        height=600,
        legend=dict(orientation="h", yanchor="bottom", y=1.02, xanchor="right", x=1),
        hovermode='closest'
    )
    
    return fig

def find_low_similarity_points(similarity_matrix, threshold=0.8):
    """
    æ‰¾å‡ºç›¸ä¼¼åº¦ä½äºé˜ˆå€¼çš„æµ‹ç‚¹
    è¿”å›æ ¼å¼: {æ–‡ä»¶å: [æµ‹ç‚¹1, æµ‹ç‚¹2, ...]}
    """
    low_similarity_results = {}
    
    for file_name in similarity_matrix.index:
        low_points = []
        for point in similarity_matrix.columns:
            similarity_value = similarity_matrix.loc[file_name, point]
            if pd.notna(similarity_value) and similarity_value < threshold:
                low_points.append(f"{point}({similarity_value:.3f})")
        
        if low_points:
            low_similarity_results[file_name] = low_points
    
    return low_similarity_results

def extract_point_names(point_list):
    """
    ä»æµ‹ç‚¹åˆ—è¡¨ä¸­æå–çº¯æµ‹ç‚¹åç§°ï¼ˆå»æ‰ç›¸ä¼¼åº¦å€¼ï¼‰
    ä¾‹å¦‚: "XM1(0.852)" -> "XM1"
    """
    point_names = []
    for point_info in point_list:
        # æå–æ‹¬å·å‰çš„æµ‹ç‚¹åç§°
        if '(' in point_info:
            point_name = point_info.split('(')[0]
            point_names.append(point_name)
        else:
            point_names.append(point_info)
    return point_names

def extract_measurement_points(dfs):
    points = set()
    for df in dfs.values():
        if df is not None and df.shape[1] >= 2:
            pts = [c for c in df.columns[1:]]
            points.update(pts)
    return points

def create_spectrum_plot_emphasis(new_dict, ref_dict, selected_point,
                                  ref_opacity=0.75, ref_line_width=1.6, ref_dash='dash',
                                  new_line_width=2.5):
    """
    åˆ›å»ºé¢‘è°±å›¾ - ä»ä¸€è‡´æ€§åˆ†ææ¨¡å—ç§»æ¤
    æ–°æ•°æ®é«˜äº®ï¼Œå‚è€ƒæ•°æ®æ·¡åŒ–
    """
    fig = go.Figure()
    colors = ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728',
              '#9467bd', '#8c564b', '#e377c2', '#bcbd22', '#17becf']

    # å‚è€ƒæ•°æ®æ·¡åŒ–ï¼ˆä½†ä¿æŒæ¸…æ™°ï¼‰
    for label, df in ref_dict.items():
        if df is None or selected_point not in df.columns:
            continue
        fig.add_trace(go.Scatter(
            x=df.iloc[:, 0], y=df[selected_point], mode='lines',
            name=f"å‚è€ƒ-{label}", line=dict(color='#7f7f7f', width=ref_line_width, dash=ref_dash),
            opacity=ref_opacity,
            hovertemplate=(
                "<b>é¢‘ç‡</b>: %{x:.2f} Hz<br>"
                "<b>å¹…å€¼</b>: %{y:.4f}<br>"
                f"<b>æ•°æ®é›†</b>: å‚è€ƒ-{label}<br>"
                f"<b>æµ‹ç‚¹</b>: {selected_point}<extra></extra>"
            )
        ))

    # æ–°æ•°æ®é«˜äº®
    for i, (label, df) in enumerate(new_dict.items()):
        if df is None or selected_point not in df.columns:
            continue
        color = colors[i % len(colors)]
        fig.add_trace(go.Scatter(
            x=df.iloc[:, 0], y=df[selected_point], mode='lines',
            name=f"æ–°-{label}", line=dict(color=color, width=new_line_width),
            hovertemplate=(
                "<b>é¢‘ç‡</b>: %{x:.2f} Hz<br>"
                "<b>å¹…å€¼</b>: %{y:.4f}<br>"
                f"<b>æ•°æ®é›†</b>: æ–°-{label}<br>"
                f"<b>æµ‹ç‚¹</b>: {selected_point}<extra></extra>"
            )
        ))

    fig.update_layout(
        title=f"é¢‘è°±åˆ†æï¼ˆæµ‹ç‚¹ï¼š{selected_point}ï¼‰",
        xaxis_title="é¢‘ç‡ (Hz)", yaxis_title="å“åº”å¹…å€¼",
        height=600,
        legend=dict(orientation="h", yanchor="bottom", y=1.02, xanchor="right", x=1),
        hovermode='closest'
    )
    fig.update_xaxes(type="log", gridcolor='lightgray', gridwidth=1, showgrid=True)
    fig.update_yaxes(type="log", gridcolor='lightgray', gridwidth=1, showgrid=True)
    return fig

# ä¾§è¾¹æ ï¼šæ•°æ®ä¸Šä¼ ä¸è½¬æ¢
st.sidebar.header("ğŸ“ ä¸Šä¼ æ•°æ®")
new_raw_files = st.sidebar.file_uploader(
    "ä¸Šä¼ æ–°çš„æœªç»å¤„ç†çš„ LMS æ•°æ®ï¼ˆ.xlsx/.xlsï¼Œå¯å¤šé€‰ï¼‰",
    type=['xlsx', 'xls'], accept_multiple_files=True,
    help="åŸå§‹ LMS å¯¼å‡ºï¼ˆå«ç¬¬12è¡Œæµ‹ç‚¹åï¼‰ï¼Œå°†è‡ªåŠ¨è½¬æ¢ä¸ºç‰¹å¾çº§æ•°æ®ã€‚"
)
ref_files = st.sidebar.file_uploader(
    "ä¸Šä¼ å‚è€ƒæ•°æ®ï¼ˆå·²å¤„ç†ç‰¹å¾çº§ Excelï¼Œç¬¬ä¸€åˆ—ä¸ºé¢‘ç‡ï¼Œå¯å¤šé€‰ï¼‰",
    type=['xlsx'], accept_multiple_files=True
)

# ä¾§è¾¹æ  - æµ‹ç‚¹ä½ç½®è¡¨ä¸Šä¼ 
st.sidebar.header("ğŸ“ æµ‹ç‚¹ä½ç½®ä¿¡æ¯")
location_file = st.sidebar.file_uploader(
    "é€‰æ‹©æµ‹ç‚¹ä½ç½®è¡¨ (.xlsx)",
    type=["xlsx"],
    help="åŒ…å«æµ‹ç‚¹èˆ±æ¿å’Œå•æœºåŒºåŸŸåˆ†å¸ƒä¿¡æ¯çš„Excelæ–‡ä»¶"
)

# ä¼šè¯çŠ¶æ€åˆå§‹åŒ–
if 'new_processed_dict' not in st.session_state:
    st.session_state.new_processed_dict = {}
if 'ref_processed_dict' not in st.session_state:
    st.session_state.ref_processed_dict = {}
if 'current_point_index' not in st.session_state:
    st.session_state.current_point_index = 0
if 'red_points' not in st.session_state:
    st.session_state.red_points = []
if 'yellow_points' not in st.session_state:
    st.session_state.yellow_points = []
if 'location_data' not in st.session_state:
    st.session_state.location_data = {}
if 'location_file_loaded' not in st.session_state:
    st.session_state.location_file_loaded = False

# è½¬æ¢æ–°æ•°æ®
st.sidebar.header("ğŸ”„ æ–°æ•°æ®è½¬æ¢")
if new_raw_files:
    if st.sidebar.button("å¼€å§‹è½¬æ¢å¹¶ç¼“å­˜", use_container_width=True):
        with st.spinner("æ­£åœ¨è½¬æ¢æ–°æ•°æ®..."):
            st.session_state.new_processed_dict.clear()
            for f in new_raw_files:
                df = convert_lms_excel(f)
                if df is not None and df.shape[1] >= 2:
                    st.session_state.new_processed_dict[f.name] = df
        st.sidebar.success(f"âœ… å·²è½¬æ¢ {len(st.session_state.new_processed_dict)} ä¸ªæ–‡ä»¶")

# å‚è€ƒæ•°æ®è¯»å–
st.sidebar.header("ğŸ“˜ å‚è€ƒæ•°æ®è¯»å–")
if ref_files:
    if st.sidebar.button("è¯»å–å‚è€ƒæ•°æ®", use_container_width=True):
        with st.spinner("æ­£åœ¨è¯»å–å‚è€ƒæ•°æ®..."):
            st.session_state.ref_processed_dict.clear()
            for f in ref_files:
                df = read_processed_excel(f)
                if df is not None and df.shape[1] >= 2:
                    st.session_state.ref_processed_dict[f.name] = df
        st.sidebar.success(f"âœ… å·²è¯»å– {len(st.session_state.ref_processed_dict)} ä¸ªå‚è€ƒæ–‡ä»¶")

# æµ‹ç‚¹ä½ç½®è¡¨è¯»å–
if location_file is not None:
    if not st.session_state.location_file_loaded or st.session_state.get('current_location_file') != location_file.name:
        with st.spinner("æ­£åœ¨åŠ è½½æµ‹ç‚¹ä½ç½®è¡¨..."):
            st.session_state.location_data = load_location_data(location_file)
            st.session_state.location_file_loaded = True
            st.session_state.current_location_file = location_file.name
            if st.session_state.location_data:
                st.sidebar.success(f"âœ… æˆåŠŸåŠ è½½æµ‹ç‚¹ä½ç½®è¡¨: {location_file.name}")
                st.sidebar.info(f"ğŸ“ å·²åŠ è½½ {len(st.session_state.location_data)} ä¸ªæµ‹ç‚¹çš„ä½ç½®ä¿¡æ¯")
            else:
                st.sidebar.warning("âš ï¸ æµ‹ç‚¹ä½ç½®è¡¨åŠ è½½å¤±è´¥æˆ–æ ¼å¼ä¸æ­£ç¡®")

# ç›¸ä¼¼åº¦åˆ†æ
st.subheader("Esimç›¸ä¼¼åº¦åˆ†æï¼ˆæ–°æ•°æ® vs å‚è€ƒæ•°æ®é›†å¹³å‡å€¼ï¼‰")
new_dict = st.session_state.new_processed_dict
ref_dict = st.session_state.ref_processed_dict

if not new_dict:
    st.info("è¯·å…ˆä¸Šä¼ å¹¶è½¬æ¢æ–°çš„æœªç»å¤„ç†æ•°æ®ã€‚")
elif not ref_dict:
    st.info("è¯·å…ˆä¸Šä¼ å‚è€ƒæ•°æ®ã€‚")
else:
    new_points = extract_measurement_points(new_dict)
    ref_points = extract_measurement_points(ref_dict)
    common_points = new_points & ref_points

    if not common_points:
        st.warning("æœªæ‰¾åˆ°å¯åœ¨æ–°ä¸å‚è€ƒä¹‹é—´å…±åŒå åŠ çš„æµ‹ç‚¹ã€‚")
    else:
        sorted_points = sorted(
            list(common_points),
            key=lambda p: (p[:2], int(re.sub(r'\D', '', p) or 0))
        )
        
        # æµ‹ç‚¹é€‰æ‹©åŒºåŸŸ
        cols = st.columns([3, 1, 1])
        with cols[0]:
            # åˆ›å»ºå¸¦é¢œè‰²æ ‡è®°å’Œä½ç½®ä¿¡æ¯çš„æµ‹ç‚¹é€‰é¡¹
            def format_point_with_color_and_location(point):
                location = get_point_location(point) if st.session_state.location_data else ""
                location_suffix = f" ({location})" if location else ""
                
                if point in st.session_state.red_points:
                    return f"ğŸ”´ {point}{location_suffix}"
                elif point in st.session_state.yellow_points:
                    return f"ğŸŸ¡ {point}{location_suffix}"
                else:
                    return f"{point}{location_suffix}"
            
            if st.session_state.location_data:
                point_options = [format_point_with_color_and_location(point) for point in sorted_points]
            else:
                point_options = [format_point_with_color_and_location(point) for point in sorted_points]
            
            selected_point_with_location = st.selectbox(
                "é€‰æ‹©æµ‹ç‚¹",
                point_options,
                index=min(st.session_state.current_point_index, len(point_options)-1)
            )
        
        # æå–åŸå§‹æµ‹ç‚¹åç§°ï¼ˆå»æ‰é¢œè‰²æ ‡è®°å’Œä½ç½®ä¿¡æ¯ï¼‰
        if st.session_state.location_data:
            # ä»å¸¦é¢œè‰²æ ‡è®°å’Œä½ç½®ä¿¡æ¯çš„é€‰é¡¹ä¸­æå–åŸå§‹æµ‹ç‚¹åç§°
            selected_point = selected_point_with_location.split(' (')[0].replace('ğŸ”´ ', '').replace('ğŸŸ¡ ', '')
        else:
            selected_point = selected_point_with_location.replace('ğŸ”´ ', '').replace('ğŸŸ¡ ', '')
        with cols[1]:
            if st.button("â¬…ï¸ ä¸Šä¸€ä¸ª", use_container_width=True):
                st.session_state.current_point_index = (st.session_state.current_point_index - 1) % len(sorted_points)
                st.rerun()
        with cols[2]:
            if st.button("â¡ï¸ ä¸‹ä¸€ä¸ª", use_container_width=True):
                st.session_state.current_point_index = (st.session_state.current_point_index + 1) % len(sorted_points)
                st.rerun()

        # åˆ›å»ºå¸¦é¢œè‰²æ ‡è®°çš„ä¸‹æ‹‰èœå•é€‰é¡¹
        def format_point_option(point):
            if point in st.session_state.red_points:
                return f"ğŸ”´ {point}"
            elif point in st.session_state.yellow_points:
                return f"ğŸŸ¡ {point}"
            else:
                return point

        formatted_points = [format_point_option(point) for point in sorted_points]
        
        # æ˜¾ç¤ºå½“å‰æµ‹ç‚¹ä¿¡æ¯
        st.info(f"å½“å‰æµ‹ç‚¹: {selected_point} ({st.session_state.current_point_index + 1}/{len(sorted_points)})")

        # é¢‘è°±åˆ†ææ˜¾ç¤º
        st.subheader("ğŸ“Š é¢‘è°±åˆ†æ")
        spectrum_fig = create_spectrum_plot_emphasis(
            new_dict, ref_dict, selected_point,
            ref_opacity=0.75, ref_line_width=1.6, ref_dash='dash',
            new_line_width=2.5
        )
        st.plotly_chart(spectrum_fig, use_container_width=True)

        # æ¸…é™¤é¢œè‰²æ ‡è®°æŒ‰é’®
        if st.button("ğŸ—‘ï¸ æ¸…é™¤æ‰€æœ‰é¢œè‰²æ ‡è®°", use_container_width=True):
            st.session_state.red_points = []
            st.session_state.yellow_points = []
            st.rerun()

        # è‡ªåŠ¨è¿›è¡Œç›¸ä¼¼åº¦åˆ†æ
        if 'similarity_matrix' not in st.session_state or st.session_state.get('current_files') != (tuple(new_dict.keys()), tuple(ref_dict.keys())):
            with st.spinner("æ­£åœ¨è¿›è¡Œç›¸ä¼¼åº¦åˆ†æ..."):
                try:
                    st.session_state.similarity_matrix, st.session_state.interpolated_data = calculate_similarity_matrix(
                        new_dict, ref_dict, sorted_points
                    )
                    st.session_state.similarity_calculated = True
                    st.session_state.current_files = (tuple(new_dict.keys()), tuple(ref_dict.keys()))
                except Exception as e:
                    st.error(f"ç›¸ä¼¼åº¦åˆ†æå¤±è´¥: {str(e)}")
                    st.session_state.similarity_calculated = False
        else:
            st.session_state.similarity_calculated = True

        # æ˜¾ç¤ºç›¸ä¼¼åº¦åˆ†æç»“æœ
        if st.session_state.similarity_calculated and 'similarity_matrix' in st.session_state:
            # ä¸»ç•Œé¢æ˜¾ç¤ºè¯¦ç»†ç»“æœ
            st.subheader("ğŸ“Š ç›¸ä¼¼åº¦ç»“æœè¡¨æ ¼")
            st.dataframe(st.session_state.similarity_matrix, use_container_width=True)

            # ç»˜åˆ¶ç›¸ä¼¼åº¦å›¾è¡¨
            st.subheader("ğŸ“ˆ ç›¸ä¼¼åº¦åˆ†æå›¾è¡¨")
            fig = plot_similarity_results(st.session_state.similarity_matrix)
            st.plotly_chart(fig, use_container_width=True)

            # æ·»åŠ ä¸‹è½½æŒ‰é’®ï¼ˆè¡¨æ ¼æ•°æ®ï¼‰
            csv_data = st.session_state.similarity_matrix.to_csv().encode('utf-8')
            st.download_button(
                label="ğŸ“¥ ä¸‹è½½CSVæ ¼å¼æ•°æ®",
                data=csv_data,
                file_name="similarity_results.csv",
                mime="text/csv"
            )


            # æ˜¾ç¤ºä¸¥é‡ä½ç›¸ä¼¼åº¦ç»“æœæ¡† (é˜ˆå€¼ 0.8)
            st.subheader("âš ï¸ ä¸¥é‡ä½ç›¸ä¼¼åº¦æµ‹ç‚¹è¯†åˆ« (ç›¸ä¼¼åº¦ < 0.8)")
            low_similarity_results_08 = find_low_similarity_points(st.session_state.similarity_matrix, threshold=0.8)

            if low_similarity_results_08:
                # æå–æ‰€æœ‰<0.8çš„æµ‹ç‚¹åç§°
                all_low_points_08 = []
                for low_points in low_similarity_results_08.values():
                    point_names = extract_point_names(low_points)
                    all_low_points_08.extend(point_names)

                # æ·»åŠ æ ‡çº¢æŒ‰é’®
                if st.button("ğŸ”´ æ ‡çº¢æ‰€æœ‰<0.8æµ‹ç‚¹", key="mark_red_08"):
                    st.session_state.red_points = list(set(st.session_state.red_points + all_low_points_08))
                    st.rerun()

                for file_name, low_points in low_similarity_results_08.items():
                    with st.expander(f"âŒ {file_name} - ä¸¥é‡ä½ç›¸ä¼¼åº¦æµ‹ç‚¹ ({len(low_points)}ä¸ª)"):
                        st.markdown("**æµ‹ç‚¹åˆ—è¡¨ (ç›¸ä¼¼åº¦å€¼):**")
                        for point_info in low_points:
                            # æ·»åŠ ä½ç½®ä¿¡æ¯æ˜¾ç¤º
                            point_with_location = format_point_with_location_and_similarity(point_info)
                            st.markdown(f"- {point_with_location}")
            else:
                st.success("ğŸ‰ æ‰€æœ‰æµ‹ç‚¹çš„ç›¸ä¼¼åº¦å‡ â‰¥ 0.8ï¼Œç»“æœä¼˜ç§€ï¼")

            # æ˜¾ç¤ºä¸­ç­‰ä½ç›¸ä¼¼åº¦ç»“æœæ¡† (é˜ˆå€¼ 0.8~0.9)
            st.subheader("âš ï¸ ä¸­ç­‰ä½ç›¸ä¼¼åº¦æµ‹ç‚¹è¯†åˆ« (ç›¸ä¼¼åº¦åœ¨ 0.8~0.9)")

            # æ‰¾å‡ºç›¸ä¼¼åº¦åœ¨0.8~0.9ä¹‹é—´çš„æµ‹ç‚¹
            medium_low_similarity_results = {}

            for file_name in st.session_state.similarity_matrix.index:
                medium_low_points = []
                for point in st.session_state.similarity_matrix.columns:
                    similarity_value = st.session_state.similarity_matrix.loc[file_name, point]
                    if pd.notna(similarity_value) and 0.8 <= similarity_value < 0.9:
                        medium_low_points.append(f"{point}({similarity_value:.3f})")

                if medium_low_points:
                    medium_low_similarity_results[file_name] = medium_low_points

            if medium_low_similarity_results:
                # æå–æ‰€æœ‰0.8~0.9çš„æµ‹ç‚¹åç§°
                all_medium_low_points = []
                for medium_low_points in medium_low_similarity_results.values():
                    point_names = extract_point_names(medium_low_points)
                    all_medium_low_points.extend(point_names)

                # æ·»åŠ æ ‡é»„æŒ‰é’®
                if st.button("ğŸŸ¡ æ ‡é»„æ‰€æœ‰0.8~0.9æµ‹ç‚¹", key="mark_yellow_08_09"):
                    st.session_state.yellow_points = list(set(st.session_state.yellow_points + all_medium_low_points))
                    st.rerun()

                for file_name, medium_low_points in medium_low_similarity_results.items():
                    with st.expander(f"ğŸ“‹ {file_name} - ä¸­ç­‰ä½ç›¸ä¼¼åº¦æµ‹ç‚¹ ({len(medium_low_points)}ä¸ª)"):
                        st.markdown("**æµ‹ç‚¹åˆ—è¡¨ (ç›¸ä¼¼åº¦å€¼):**")
                        for point_info in medium_low_points:
                            # æ·»åŠ ä½ç½®ä¿¡æ¯æ˜¾ç¤º
                            point_with_location = format_point_with_location_and_similarity(point_info)
                            st.markdown(f"- {point_with_location}")
            else:
                st.success("ğŸ‰ æ‰€æœ‰æµ‹ç‚¹çš„ç›¸ä¼¼åº¦å‡ â‰¥ 0.9 æˆ– < 0.8ï¼Œç»“æœè‰¯å¥½ï¼")

            st.success("âœ… ç›¸ä¼¼åº¦åˆ†æå·²å®Œæˆï¼")
        else:
            st.warning("è¯·è‡³å°‘ä¸Šä¼ ä¸€ä¸ªæœ‰æ•ˆçš„æ–°æ•°æ®å’Œå‚è€ƒæ•°æ®")

# è¿”å›ä¸»é¡µé¢çš„å¯¼èˆª
st.markdown("---")
st.sidebar.markdown("---")
if st.sidebar.button("ğŸ  è¿”å›ä¸»é¡µé¢"):
    st.switch_page("main_app.py")

# é¡µè„šä¿¡æ¯
st.markdown("---")
st.caption("ç›¸ä¼¼åº¦åˆ†æç³»ç»Ÿ | åŸºäºStreamlitå’ŒPlotlyå¼€å‘ | ä½¿ç”¨å‚è€ƒæ•°æ®é›†å¹³å‡å€¼ä½œä¸ºåŸºå‡†å‘é‡")

# åœ¨æ¨¡å—æœ€åæ˜¾ç¤ºEsimå…¬å¼è¯´æ˜
st.markdown("---")
st.subheader("ğŸ“ Esimç›¸ä¼¼åº¦å…¬å¼è¯´æ˜")

# æ˜¾ç¤ºå…¬å¼å›¾ç‰‡
try:
    formula_image = Image.open('esim_formula.png')
    st.image(formula_image, caption="Esimç›¸ä¼¼åº¦è®¡ç®—å…¬å¼", use_container_width=True)
except FileNotFoundError:
    st.warning("Esimå…¬å¼å›¾ç‰‡æœªæ‰¾åˆ°ï¼Œè¯·ç¡®ä¿esim_formula.pngæ–‡ä»¶å­˜åœ¨")

# æ˜¾ç¤ºå…¬å¼è¯´æ˜
st.markdown("""
**å…¬å¼è¯´æ˜ï¼š**

- **X**: å‚è€ƒæ•°æ®é›†å¹³å‡å€¼å‘é‡ï¼ˆæ‰€æœ‰å‚è€ƒæ•°æ®åœ¨æŸæµ‹ç‚¹çš„å¹³å‡å€¼ï¼‰
- **Y**: æ–°æ•°æ®æµ‹ç‚¹æ•°æ®å‘é‡ï¼ˆæ–°æ•°æ®æ–‡ä»¶åœ¨æŸæµ‹ç‚¹çš„æ•°æ®ï¼‰
- **ç›¸ä¼¼åº¦**: åŸºäºEsimå…¬å¼è®¡ç®—çš„ç›¸ä¼¼åº¦å€¼ï¼ŒèŒƒå›´åœ¨0-1ä¹‹é—´
- **n**: é¢‘ç‡ç‚¹æ•°
- **Ï‰_i**: æƒé‡ç³»æ•°ï¼ˆå½“å‰ä½¿ç”¨ç­‰æƒé‡1ï¼‰
- **x_i, y_i**: ç¬¬iä¸ªé¢‘ç‡ç‚¹çš„å¹…å€¼

**å…¬å¼ç‰¹ç‚¹ï¼š**
- ä½¿ç”¨æŒ‡æ•°è¡°å‡å‡½æ•°åº¦é‡ç›¸ä¼¼åº¦
- è€ƒè™‘äº†ç»å¯¹å·®å¼‚å’Œå¹³å‡å€¼çš„ç›¸å¯¹å…³ç³»
- å½“X=Yæ—¶ï¼Œç›¸ä¼¼åº¦=1ï¼ˆå®Œå…¨ç›¸ä¼¼ï¼‰
- å½“Xå’ŒYå·®å¼‚å¾ˆå¤§æ—¶ï¼Œç›¸ä¼¼åº¦è¶‹è¿‘äº0
""")
